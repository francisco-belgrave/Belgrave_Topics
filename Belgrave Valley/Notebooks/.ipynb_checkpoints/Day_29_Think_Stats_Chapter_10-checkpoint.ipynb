{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<img src=\"https://user-images.strikinglycdn.com/res/hrscywv4p/image/upload/c_limit,fl_lossy,h_300,w_300,f_auto,q_auto/1266110/Logo_wzxi0f.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "**Be less curious about people and more curious about ideas - [Marie Curie](https://en.wikipedia.org/wiki/Marie_Curie)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "# Chapter 10 Linear least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## 1. Least squares fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\n",
    "Correlation coefficients measure the strength and sign of a relationship, but not the slope. There are several ways to estimate the slope; the most common is a **linear least squares** fit. A “linear fit” is a line intended to model the relationship between variables. A “least squares” fit is one that minimizes the mean squared error (MSE) between the line and the data.\n",
    "\n",
    "Suppose we have a sequence of points, ys, that we want to express as a function of another sequence xs. If there is a linear relationship between xs and ys with intercept inter and slope slope, we expect each y[i] to be inter + slope * x[i].\n",
    "\n",
    "But unless the correlation is perfect, this prediction is only approximate. The vertical deviation from the line, or **residual**, is\n",
    "\n",
    "res = ys - (inter + slope * xs)\n",
    "\n",
    "The residuals might be due to random factors like measurement error, or non-random factors that are unknown. For example, if we are trying to predict weight as a function of height, unknown factors might include diet, exercise,\n",
    "and body type.\n",
    "\n",
    "If we get the parameters inter and slope wrong, the residuals get bigger, so it makes intuitive sense that the parameters we want are the ones that minimize the residuals.\n",
    "\n",
    "We might try to minimize the absolute value of the residuals, or their squares, or their cubes; but the most common choice is to minimize the sum of squared residuals, sum(res**2)).\n",
    "\n",
    "Why? There are three good reasons and one less important one:\n",
    "\n",
    "- Squaring has the feature of treating positive and negative residuals the same, which is usually what we want.\n",
    "\n",
    "- Squaring gives more weight to large residuals, but not so much weight that the largest residual always dominates.\n",
    "\n",
    "- If the residuals are uncorrelated and normally distributed with mean 0 and constant (but unknown) variance, then the least squares fit is also the maximum likelihood estimator of inter and slope. See https://en.wikipedia.org/wiki/Linear_regression.\n",
    "\n",
    "- The values of inter and slope that minimize the squared residuals can be computed efficiently.\n",
    "\n",
    "The last reason made sense when computational efficiency was more important than choosing the method most appropriate to the problem at hand. That’s no longer the case, so it is worth considering whether squared residuals are the right thing to minimize.\n",
    "\n",
    "For example, if you are using xs to predict values of ys, guessing too high might be better (or worse) than guessing too low. In that case you might want to compute some cost function for each residual, and minimize total\n",
    "cost, sum(cost(res)). However, computing a least squares fit is quick, easy and often good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## 2. Implementatio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "`thinkstats2` provides simple functions that demonstrate linear least squares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "7iOL0rgTU5m4"
   },
   "outputs": [],
   "source": [
    "from Resources.Think_Stats.Thinkstats2 import first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "GGXDDzKIU5nD"
   },
   "outputs": [],
   "source": [
    "from Resources.Think_Stats.Thinkstats2 import thinkstats2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "bnreZBqNU5nH"
   },
   "outputs": [],
   "source": [
    "live, firsts, others = first.MakeFrames()\n",
    "live = live.dropna(subset=['agepreg', 'totalwgt_lb'])\n",
    "ages = live.agepreg\n",
    "weights = live.totalwgt_lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def LeastSquares(xs, ys):\n",
    "    meanx, varx = thinkstats2.MeanVar(xs)\n",
    "    meany = thinkstats2.Mean(ys)\n",
    "    slope = thinkstats2.Cov(xs, ys, meanx, meany) / varx\n",
    "    inter = meany - slope * meanx\n",
    "    return inter, slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "`LeastSquares` takes sequences xs and ys and returns the estimated parameters inter and slope. For details on how it works, see http://wikipedia.org/wiki/Numerical_methods_for_linear_least_squares.\n",
    "\n",
    "`thinkstats2` also provides `FitLine`, which takes inter and slope and returns the fitted line for a sequence of xs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def FitLine(xs, inter, slope):\n",
    "    fit_xs = np.sort(xs)\n",
    "    fit_ys = inter + slope * fit_xs\n",
    "    return fit_xs, fit_ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Use these functions to compute the least squares fit for birth weight as a function of mother’s age:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#Code it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Draw a scatter plot of the two variables and plot a line on top of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#Code it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The estimated intercept and slope are 6.8 lbs and 0.017 lbs per year. These values are hard to interpret in this form: the intercept is the expected weight of a baby whose mother is 0 years old, which doesn’t make sense in context, and the slope is too small to grasp easily.\n",
    "\n",
    "Instead of presenting the intercept at x = 0, it is often helpful to present the intercept at the mean of x. In this case the mean age is about 25 years and the mean baby weight for a 25 year old mother is 7.3 pounds. The slope is 0.27 ounces per year, or 0.17 pounds per decade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## 3. Residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Another useful test is to plot the residuals.\n",
    "\n",
    "Given two sequences xs, ys and the regression line with given inter and slope calculated before create a function that calculates the residuals. Knowing:\n",
    "\n",
    "res = ys - (inter + slope * xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#Code it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "To visualize the residuals, I'll split the respondents into groups by age, then plot the percentiles of the residuals versus the average age in each group.\n",
    "\n",
    "First I'll make the groups and compute the average age in each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "bins = np.arange(10, 48, 3)\n",
    "indices = np.digitize(live.agepreg, bins)\n",
    "groups = live.groupby(indices)\n",
    "\n",
    "age_means = [group.agepreg.mean() for _, group in groups][1:-1]\n",
    "age_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Next I'll compute the CDF of the residuals in each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "cdfs = [thinkstats2.Cdf(group.residual) for _, group in groups][1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The following function plots percentiles of the residuals against the average age in each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def PlotPercentiles(age_means, cdfs):\n",
    "    thinkplot.PrePlot(3)\n",
    "    for percent in [75, 50, 25]:\n",
    "        weight_percentiles = [cdf.Percentile(percent) for cdf in cdfs]\n",
    "        label = '%dth' % percent\n",
    "        thinkplot.Plot(age_means, weight_percentiles, label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The following figure shows the 25th, 50th, and 75th percentiles.\n",
    "\n",
    "Curvature in the residuals suggests a non-linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from Resources.Think_Stats.Thinkstats2 import thinkplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "PlotPercentiles(age_means, cdfs)\n",
    "\n",
    "thinkplot.Config(xlabel=\"Mother's age (years)\",\n",
    "                 ylabel='Residual (lbs)',\n",
    "                 xlim=[10, 45])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "To visualize the residuals, I group respondents by age and compute percentiles in each group. Then show the 25th, 50th and 75th percentiles of the residuals for each age group. The median is near zero, as expected, and the interquartile range is about 2 pounds. So if we know the mother’s age, we can guess the baby’s weight within a pound, about 50% of the time.\n",
    "\n",
    "Ideally these lines should be flat, indicating that the residuals are random, and parallel, indicating that the variance of the residuals is the same for all age groups. In fact, the lines are close to parallel, so that’s good; but they have some curvature, indicating that the relationship is nonlinear. Nevertheless, the linear fit is a simple model that is probably good enough for some purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## 4. Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\n",
    "The parameters slope and inter are estimates based on a sample; like other estimates, they are vulnerable to sampling bias, measurement error, and sampling error. As discussed in Chapter 8, sampling bias is caused\n",
    "by non-representative sampling, measurement error is caused by errors in collecting and recording data, and sampling error is the result of measuring a sample rather than the entire population.\n",
    "\n",
    "To assess sampling error, we ask, “If we run this experiment again, how much variability do we expect in the estimates?” We can answer this question by running simulated experiments and computing sampling distributions of the estimates.\n",
    "\n",
    "I simulate the experiments by resampling the data; that is, I treat the observed pregnancies as if they were the entire population and draw samples, with replacement, from the observed sample.\n",
    "\n",
    "Make a simulation recalculating slope and iter of 100 random picks from live data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Code it here\n",
    "\n",
    "##Thinkstats resampling method is probably useful\n",
    "sample = thinkstats2.ResampleRows(live)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "After resampling, we use the simulated sample to estimate parameters. The result is two sequences: the estimated intercepts and estimated slopes. You can summarize the sampling distributions by printing the standard deviation and\n",
    "confidence interval of the estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#Code it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The following function takes a list of estimates and prints the mean, standard error, and 90% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def Summarize(estimates, actual=None):\n",
    "    mean = Mean(estimates)\n",
    "    stderr = Std(estimates, mu=actual)\n",
    "    cdf = thinkstats2.Cdf(estimates)\n",
    "    ci = cdf.ConfidenceInterval(90)\n",
    "    print('mean, SE, CI', mean, stderr, ci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Here's  the summary for `inter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean, SE, CI 6.82902330267 0.0704099872199 (6.7111715080534804, 6.9409762759895068)\n"
     ]
    }
   ],
   "source": [
    "Summarize(inters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "And for `slope`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean, SE, CI 0.0175060534792 0.00281785466975 (0.013019789706016532, 0.022194850584772059)\n"
     ]
    }
   ],
   "source": [
    "Summarize(slopes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "For the intercept, the mean estimate is 6.83, with standard deviation 0.07 and 90% confidence interval (6.71, 6.94). The estimated slope, in more compact form, is 0.0174, SD 0.0028, CI (0.0126, 0.0220). There is almost a factor of\n",
    "two between the low and high ends of this CI, so it should be considered a rough estimate.\n",
    "\n",
    "To visualize the sampling error of the estimate, we could plot all of the fitted lines, or for a less cluttered representation, plot a 90% confidence interval for each age. Here’s the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def PlotConfidenceIntervals(xs, inters, slopes,\n",
    "    percent=90, **options):\n",
    "    fys_seq = []\n",
    "    \n",
    "    for inter, slope in zip(inters, slopes):\n",
    "        fxs, fys = thinkstats2.FitLine(xs, inter, slope)\n",
    "        fys_seq.append(fys)\n",
    "        \n",
    "    p = (100 - percent) / 2\n",
    "    percents = p, 100 - p\n",
    "    low, high = thinkstats2.PercentileRows(fys_seq, percents)\n",
    "    thinkplot.FillBetween(fxs, low, high, **options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "xs is the sequence of mother’s age. inters and slopes are the estimated parameters generated by Sampling Distributions. percent indicates which confidence interval to plot.\n",
    "\n",
    "`PlotConfidenceIntervals` generates a fitted line for each pair of inter and slope and stores the results in a sequence, fys_seq. Then it uses `PercentileRows` to select the upper and lower percentiles of y for each value of x. For a 90% confidence interval, it selects the 5th and 95th percentiles. `FillBetween` draws a polygon that fills the space between two lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## 5. Godness of fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "There are several ways to measure the quality of a linear model, or **goodness of fit**. One of the simplest is the standard deviation of the residuals.\n",
    "\n",
    "If you use a linear model to make predictions, Std(res) is the root mean squared error (RMSE) of your predictions. For example, if you use mother’s age to guess birth weight, the RMSE of your guess would be 1.40 lbs. \n",
    "\n",
    "Guess now mother age without the weight of the mother as a guess and calculate the RMSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Code it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "If you guess birth weight without knowing the mother’s age, the RMSE of your guess is Std(ys), which is 1.41 lbs. So in this example, knowing a mother’s age does not improve the predictions substantially.\n",
    "\n",
    "Another way to measure goodness of fit is the coefficient of determination, usually denoted R^2 and called **“R-squared”**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def CoefDetermination(ys, res):\n",
    "    return 1 - thinkstats2.Var(res) / thinkstats2.Var(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Var(res) is the MSE of your guesses using the model, Var(ys) is the MSE without it. So their ratio is the fraction of MSE that remains if you use the model, and R^2 is the fraction of MSE the model eliminates.\n",
    "\n",
    "What is the R^2 for birth weight and mother’s age? And how do you interpret it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#Code it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "There is a simple relationship between the coefficient of determination and Pearson’s coefficient of correlation: R^2 = ρ^2 . For example, if ρ is 0.8 or -0.8, R^2 = 0.64.\n",
    "\n",
    "Although ρ and R^2 are often used to quantify the strength of a relationship, they are not easy to interpret in terms of predictive power. In my opinion, Std(res) is the best representation of the quality of prediction, especially if it is presented in relation to Std(ys).\n",
    "\n",
    "For example, when people talk about the validity of the SAT (a standardized test used for college admission in the U.S.) they often talk about correlations between SAT scores and other measures of intelligence.\n",
    "\n",
    "According to one study, there is a Pearson correlation of ρ = 0.72 between total SAT scores and IQ scores, which sounds like a strong correlation. But R^2 = ρ^2 = 0.52, so SAT scores account for only 52% of variance in IQ.\n",
    "\n",
    "IQ scores are normalized with Std(ys) = 15, so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "var_ys = 15**2\n",
    "rho = 0.72\n",
    "r2 = rho**2\n",
    "var_res = (1 - r2) * var_ys\n",
    "std_res = math.sqrt(var_res)\n",
    "\n",
    "std_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "So using SAT score to predict IQ reduces RMSE from 15 points to 10.4 points. A correlation of 0.72 yields a reduction in RMSE of only 31%.\n",
    "\n",
    "If you see a correlation that looks impressive, remember that R^2 is a better indicator of reduction in MSE, and reduction in RMSE is a better indicator of predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## 6. Testing a linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The effect of mother’s age on birth weight is small, and has little predictive power. So is it possible that the apparent relationship is due to chance? There are several ways we might test the results of a linear fit.\n",
    "\n",
    "One option is to test whether the apparent reduction in MSE is due to chance. In that case, the test statistic is R^2 and the null hypothesis is that there is no relationship between the variables. We can simulate the null hypothesis by permutation, as in Section 9.5, when we tested the correlation between mother’s age and birth weight. In fact, because R^2 = ρ , a one-sided test of R^2 is equivalent to a two-sided test of ρ. We’ve already done that test, and found p < 0.001, so we conclude that the apparent relationship between mother’s age and birth weight is statistically significant.\n",
    "\n",
    "Another approach is to test whether the apparent slope is due to chance. The null hypothesis is that the slope is actually zero; in that case we can model the birth weights as random variations around their mean. Here’s a\n",
    "HypothesisTest for this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class SlopeTest(thinkstats2.HypothesisTest):\n",
    "    def TestStatistic(self, data):\n",
    "        ages, weights = data\n",
    "        _, slope = thinkstats2.LeastSquares(ages, weights)\n",
    "        return slope\n",
    "    \n",
    "    def MakeModel(self):\n",
    "        _, weights = self.data\n",
    "        self.ybar = weights.mean()\n",
    "        self.res = weights - self.ybar\n",
    "        \n",
    "    def RunModel(self):\n",
    "        ages, _ = self.data\n",
    "        weights = self.ybar + np.random.permutation(self.res)\n",
    "        return ages, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The data are represented as sequences of ages and weights. The test statistic is the slope estimated by `LeastSquares`. The model of the null hypothesis is represented by the mean weight of all babies and the deviations from the mean. To generate simulated data, we permute the deviations and add them\n",
    "to the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "live, firsts, others = first.MakeFrames()\n",
    "live = live.dropna(subset=['agepreg', 'totalwgt_lb'])\n",
    "ht = SlopeTest((live.agepreg, live.totalwgt_lb))\n",
    "pvalue = ht.PValue()\n",
    "pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The p-value is less than 0.001, so although the estimated slope is small, it is unlikely to be due to chance.\n",
    "\n",
    "Estimating the p-value by simulating the null hypothesis is strictly correct, but there is a simpler alternative. Remember that we already computed the sampling distribution of the slope, in Section 10.4. To do that, we assumed\n",
    "that the observed slope was correct and simulated experiments by resampling.\n",
    "\n",
    "Before we calculated the sampling distribution of the slope, from Section 10.4, and the distribution of slopes generated under the null hypothesis. The sampling distribution is centered about the estimated slope, 0.017 lbs/year, and the slopes under the null hypothesis are centered around 0; but other than that, the distributions are identical. The distributions are also symmetric, for reasons we will see in Section 14.4.\n",
    "\n",
    "So we could estimate the p-value two ways:\n",
    "\n",
    "1. Compute the probability that the slope under the null hypothesis exceeds the observed slope.\n",
    "\n",
    "2. Compute the probability that the slope in the sampling distribution falls below 0. (If the estimated slope were negative, we would compute the probability that the slope in the sampling distribution exceeds 0.)\n",
    "\n",
    "The second option is easier because we normally want to compute the sampling distribution of the parameters anyway. And it is a good approximation unless the sample size is small and the distribution of residuals is skewed. Even then, it is usually good enough, because p-values don’t have to be precise.\n",
    "\n",
    "Here’s the code that estimates the p-value of the slope using the sampling distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "iK5MrTk1U5pZ"
   },
   "outputs": [],
   "source": [
    "def SamplingDistributions(live, iters=101):\n",
    "    t = []\n",
    "    for _ in range(iters):\n",
    "        sample = thinkstats2.ResampleRows(live)\n",
    "        ages = sample.agepreg\n",
    "        weights = sample.totalwgt_lb\n",
    "        estimates = thinkstats2.LeastSquares(ages, weights)\n",
    "        t.append(estimates)\n",
    "\n",
    "    inters, slopes = zip(*t)\n",
    "    return inters, slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "uXT6lngcU5pc",
    "outputId": "9bbd1e32-384a-429b-ca5a-8e8cd9f0d2b0"
   },
   "outputs": [],
   "source": [
    "inters, slopes = SamplingDistributions(live, iters=1001)\n",
    "slope_cdf = thinkstats2.Cdf(slopes)\n",
    "pvalue = slope_cdf[0]\n",
    "pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "diJbQE-9U5pe",
    "outputId": "78e41c93-5c4c-4da9-b587-bf4c45fa277e"
   },
   "outputs": [],
   "source": [
    "slope_cdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Again, we find p < 0.001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## 7. Weighted resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "So far we have treated the NSFG data as if it were a representative sample, but as said in Chapter one it is not. The survey deliberately oversamples several groups in order to improve the chance of getting statistically\n",
    "significant results; that is, in order to improve the power of tests involving these groups.\n",
    "\n",
    "This survey design is useful for many purposes, but it means that we cannot use the sample to estimate values for the general population without accounting for the sampling process.\n",
    "\n",
    "For each respondent, the NSFG data includes a variable called `finalwgt`, which is the number of people in the general population the respondent represents. This value is called a sampling weight, or just “weight.”\n",
    "\n",
    "As an example, if you survey 100,000 people in a country of 300 million, each respondent represents 3,000 people. If you oversample one group by a factor of 2, each person in the oversampled group would have a lower weight, about\n",
    "1500.\n",
    "\n",
    "To correct for oversampling, we can use resampling; that is, we can draw samples from the survey using probabilities proportional to sampling weights. Then, for any quantity we want to estimate, we can generate sampling distributions, standard errors, and confidence intervals. As an example, I will estimate mean birth weight with and without sampling weights.\n",
    "\n",
    "In Section 4, we saw `ResampleRows`, which chooses rows from a `DataFrame`, giving each row the same probability. Now we need to do the same thing using probabilities proportional to sampling weights. `ResampleRowsWeighted` takes a `DataFrame`, resamples rows according to the weights in `finalwgt`, and returns a `DataFrame` containing the resampled\n",
    "rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def ResampleRowsWeighted(df, column='finalwgt'):\n",
    "    weights = df[column]\n",
    "    cdf = Cdf(dict(weights))\n",
    "    indices = cdf.Sample(len(weights))\n",
    "    sample = df.loc[indices]\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "weights is a Series; converting it to a dictionary makes a map from the indices to the weights. In cdf the values are indices and the probabilities are proportional to the weights.\n",
    "\n",
    "indices is a sequence of row indices; sample is a DataFrame that contains the selected rows. Since we sample with replacement, the same row might appear more than once.\n",
    "\n",
    "Now we can compare the effect of resampling with and without weights. Without weights, we generate the sampling distribution like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "iters = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "estimates = [thinkstats2.ResampleRows(live).totalwgt_lb.mean()\n",
    "            for _ in range(iters)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "With weights, it looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "estimates_weighted = [ResampleRowsWeighted(live).totalwgt_lb.mean()\n",
    "            for _ in range(iters)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Calculate the mean of birth weight, the standard error and the 90% CI of both distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Code it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The following table summarizes the results:\n",
    "\n",
    "![alt text](Resources/Think_Stats/notebookpics/table_resa.png \"Title\")\n",
    "    \n",
    "In this example, the effect of weighting is small but non-negligible. The difference in estimated means, with and without weighting, is about 0.08 pounds, or 1.3 ounces. This difference is substantially larger than the standard error of the estimate, 0.014 pounds, which implies that the difference is not due to chance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
