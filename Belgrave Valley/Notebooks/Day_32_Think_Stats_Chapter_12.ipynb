{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "yoESwIwNM0c9"
   },
   "source": [
    "<img src=\"https://user-images.strikinglycdn.com/res/hrscywv4p/image/upload/c_limit,fl_lossy,h_300,w_300,f_auto,q_auto/1266110/Logo_wzxi0f.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "**Solo el que intenta el absurdo es capaz de lograr lo imposible - [Miguel de Unamuno](https://es.wikipedia.org/wiki/Miguel_de_Unamuno)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Only who attempts the absurd is capable of achieving the impossible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "# Chapter 12 Time Series Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "A **time series** is a sequence of measurements from a system that varies in time. One famous example is the “hockey stick graph” that shows global average temperature over time (see https://en.wikipedia.org/wiki/Hockey_stick_graph)\n",
    "\n",
    "The example I work with in this chapter comes from Zachary M. Jones, a researcher in political science who studies the black market for cannabis in the U.S. (http://zmjones.com/marijuana). He collected data from a web site called “Price of Weed” that crowdsources market information by asking participants to report the price, quantity, quality, and location of cannabis transactions (http://www.priceofweed.com/). The goal of his project is to investigate the effect of policy decisions, like legalization, on markets. I find this project appealing because it is an example that uses data to address important political questions, like drug policy.\n",
    "\n",
    "I hope you will find this chapter interesting, but I’ll take this opportunity to reiterate the importance of maintaining a professional attitude to data analysis. Whether and which drugs should be illegal are important and\n",
    "difficult public policy questions; our decisions should be informed by accurate data reported honestly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Importing and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The data I downloaded from Mr. Jones’s site is in the repository for this book. The following code reads it into a pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "transactions = pd.read_csv('Resources/Think_Stats/Thinkstats2/mj-clean.csv', parse_dates=[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "`parse_dates` tells `read_csv` to interpret values in column 5 as dates and convert them to NumPy datetime64 objects.\n",
    "\n",
    "Do some initial analysis at analysis on the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Code it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The dataset has the following columns:\n",
    "\n",
    "- city: string city name.\n",
    "- state: two-letter state abbreviation.\n",
    "- price: price paid in dollars.\n",
    "- amount: quantity purchased in grams.\n",
    "- quality: high, medium, or low quality, as reported by the purchaser.\n",
    "- date: date of report, presumed to be shortly after date of purchase.\n",
    "- ppg: price per gram, in dollars.\n",
    "- state.name: string state name.\n",
    "- lat: approximate latitude of the transaction, based on city name.\n",
    "- lon: approximate longitude of the transaction.\n",
    "\n",
    "Each transaction is an event in time, so we could treat this dataset as a time series. But the events are not equally spaced in time; the number of transactions reported each day varies from 0 to several hundred. Many\n",
    "methods used to analyze time series require the measurements to be equally spaced, or at least things are simpler if they are.\n",
    "\n",
    "Divide the dataset into groups by average reported quality. Use the python groupby method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Code it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Create a result Dataframes with average price per day for each quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Code it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Using your knowledge of Matplotlib plot a scaterplot of the series of quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Code it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Although there are methods specific to time series analysis, for many problems a simple way to get started is by applying general-purpose tools like linear regression. \n",
    "Create a function that takes a DataFrame of daily prices and computes a least squares fit, returning the model and results objects from `StatsModels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Code it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "What do you see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The model seems like a good linear fit for the data; nevertheless, linear regression is not the most appropriate choice for this data:\n",
    "\n",
    "- First, there is no reason to expect the long-term trend to be a line or any other simple function. In general, prices are determined by supply and demand, both of which vary over time in unpredictable ways.\n",
    "\n",
    "- Second, the linear regression model gives equal weight to all data, recent and past. For purposes of prediction, we should probably give more weight to recent data.\n",
    "\n",
    "- Finally, one of the assumptions of linear regression is that the residuals are uncorrelated noise. With time series data, this assumption is often false because successive values are correlated.\n",
    "\n",
    "The next section presents an alternative that is more appropriate for time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Moving Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Most time series analysis is based on the modeling assumption that the observed series is the sum of **three components**:\n",
    "\n",
    "- Trend: A smooth function that captures persistent changes.\n",
    "- Seasonality: Periodic variation, possibly including daily, weekly, monthly, or yearly cycles.\n",
    "- Noise: Random variation around the long-term trend.\n",
    "\n",
    "Regression is one way to extract the trend from a series, as we saw in the previous section. But if the trend is not a simple function, a good alternative is a **moving average**. A moving average divides the series into overlapping regions, called **windows**, and computes the average of the values in each window.\n",
    "\n",
    "One of the simplest moving averages is the **rolling mean**, which computes the mean of the values in each window. For example, if the window size is 3, the rolling mean computes the mean of values 0 through 2, 1 through 3, 2\n",
    "through 4, etc.\n",
    "\n",
    "pandas provides rolling_mean, which takes a Series and a window size and returns a new Series.\n",
    "\n",
    "Calculate and plot the rolling mean with a window of size 2,5,15, 30 and 60 and for each of the values series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Code it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "An alternative is the **exponentially-weighted moving average (EWMA)**, which has two advantages. First, as the name suggests, it computes a weighted average where the most recent value has the highest weight and the weights for previous values drop off exponentially. Second, the pandas implementation of EWMA handles missing values better.\n",
    "\n",
    "Do the same implementation and plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Code it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The span parameter corresponds roughly to the window size of a moving average; it controls how fast the weights drop off, so it determines the number of points that make a non-negligible contribution to each average.\n",
    "\n",
    "The values are noisy at the beginning of\n",
    "the time series, because they are based on fewer data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Now that we have characterized the trend of the time series, the next step is to investigate seasonality, which is periodic behavior. Time series data based on human behavior often exhibits daily, weekly, monthly, or yearly cycles. In the next section I present methods to test for seasonality, but they don’t work well with missing data, so we have to solve that problem first.\n",
    "\n",
    "A simple and common way to fill missing data is to use a moving average. The Series method fillna does just what we want. Fill the na in the dataframe with the average means you calculated before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Code it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "A drawback of this method is that it understates the noise in the series. We can solve that problem by adding in resampled residuals.\n",
    "\n",
    "Calculate the residuals obtained on the rest of the dataset between the series and a given point and pick a random one to add to the series to each of the points calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Code it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Serial correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "As prices vary from day to day, you might expect to see patterns. If the price is high on Monday, you might expect it to be high for a few more days; and if it’s low, you might expect it to stay low. A pattern like this is called **serial correlation**, because each value is correlated with the next one in the series.\n",
    "\n",
    "To compute serial correlation, we can shift the time series by an interval called a **lag**, and then compute the correlation of the shifted series with the original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def SerialCorr(series, lag=1):\n",
    "    xs = series[lag:]\n",
    "    ys = series.shift(lag)[lag:]\n",
    "    corr = thinkstats2.Corr(xs, ys)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "After the shift, the first lag values are nan, so I use a slice to remove them before computing Corr.\n",
    "\n",
    "If we apply SerialCorr to the raw price data with lag 1, we find serial correlation 0.48 for the high quality category, 0.16 for medium and 0.10 for low. In any time series with a long-term trend, we expect to see strong serial correlations; for example, if prices are falling, we expect to see values above the mean in the first half of the series and values below the mean in the second half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Code it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "It is more interesting to see if the correlation persists if you subtract away the trend. For example, we can compute the residual of the EWMA and then compute its serial correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "ewma = pd.ewma(reindexed.ppg, span=30)\n",
    "resid = reindexed.ppg - ewma\n",
    "corr = SerialCorr(resid, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "With lag=1, the serial correlations for the de-trended data are -0.022 for high quality, -0.015 for medium, and 0.036 for low. These values are small, indicating that there is little or no one-day serial correlation in this series.\n",
    "\n",
    "Run the analysis again with differenst lags of 1, 7, 30 and 365 to see the correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Code it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "In the next section we’ll test whether these correlations are statistically significant (they are not), but at this point we can tentatively conclude that there are no substantial seasonal patterns in these series, at least not with these lags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "If you think a series might have some serial correlation, but you don’t know which lags to test, you can test them all! The **autocorrelation function** is a function that maps from lag to the serial correlation with the given lag. “Autocorrelation” is another name for serial correlation, used more often when the lag is not 1.\n",
    "\n",
    "StatsModels, which we used for linear regression also provides functions for time series analysis, including acf, which computes the autocorrelation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import statsmodels.tsa.stattools as smtsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "acf = smtsa.acf(filled.resid, nlags=365, unbiased=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "`acf` computes serial correlations with lags from 0 through nlags. The unbiased flag tells acf to correct the estimates for the sample size. The result is an array of correlations. If we select daily prices for high quality,\n",
    "and extract correlations for lags 1, 7, 30, and 365, we can confirm that acf and SerialCorr yield approximately the same results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "acf[0], acf[1], acf[7], acf[30], acf[365]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "With `lag=0`, `acf` computes the correlation of the series with itself, which is always 1.\n",
    "Plot the autocorrelation function for a lag of 40 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Code it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "To see what the autocorrelation function looks like when there is a seasonal component, I generated simulated data by adding a weekly cycle. Assuming that demand for cannabis is higher on weekends, we might expect the price\n",
    "to be higher. To simulate this effect, I select dates that fall on Friday or Saturday and add a random amount to the price, chosen from a uniform distribution from `$0 to `$2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def AddWeeklySeasonality(daily):\n",
    "    frisat = (daily.index.dayofweek==4) | (daily.index.dayofweek==5)\n",
    "    fake = daily.copy()\n",
    "    fake.ppg[frisat] += np.random.uniform(0, 2, frisat.sum())\n",
    "    return fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "plot the autocorrelation of this new series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Code it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "autocorrelation functions for prices with this simulated seasonality. As expected, the correlations are highest when the lag is a multiple of 7. For high and medium quality, the new correlations are statistically significant. For low quality they are not, because residuals in this category are large; the effect would have to be bigger to be visible through the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Time series analysis can be used to investigate, and sometimes explain, the behavior of systems that vary in time. It can also make predictions.\n",
    "\n",
    "The linear regressions we used in Section 12.3 can be used for prediction. The RegressionResults class provides predict, which takes a DataFrame containing the explanatory variables and returns a sequence of predictions.\n",
    "Here’s the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def GenerateSimplePrediction(results, years):\n",
    "    n = len(years)\n",
    "    inter = np.ones(n)\n",
    "    d = dict(Intercept=inter, years=years)\n",
    "    predict_df = pandas.DataFrame(d)\n",
    "    predict = results.predict(predict_df)\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "results is a RegressionResults object; years is the sequence of time values we want predictions for. The function constructs a DataFrame, passes it to predict, and returns the result.\n",
    "\n",
    "If all we want is a single, best-guess prediction, we’re done. But for most purposes it is important to quantify error. In other words, we want to know how accurate the prediction is likely to be.\n",
    "\n",
    "**In modeling there are three sources of error** we should take into account:\n",
    "\n",
    "- Sampling error: The prediction is based on estimated parameters, which depend on random variation in the sample. If we run the experiment again, we expect the estimates to vary.\n",
    "\n",
    "- Random variation: Even if the estimated parameters are perfect, the observed data varies randomly around the long-term trend, and we expect this variation to continue in the future.\n",
    "\n",
    "- Modeling error: We have already seen evidence that the long-term trend is not linear, so predictions based on a linear model will eventually fail.\n",
    "\n",
    "Another source of error to consider is unexpected future events. Agricultural prices are affected by weather, and all prices are affected by politics and law. As I write this, cannabis is legal in two states and legal for medical purposes in 20 more. If more states legalize it, the price is likely to go down. But if the federal government cracks down, the price might go up.\n",
    "\n",
    "Modeling errors and unexpected future events are hard to quantify. Sampling error and random variation are easier to deal with, so we’ll do that first.\n",
    "\n",
    "To quantify sampling error, I use resampling, as we did in before. As always, the goal is to use the actual observations to simulate what would happen if we ran the experiment again. The simulations are based on the\n",
    "assumption that the estimated parameters are correct, but the random residuals could have been different. Here is a function that runs the simulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def SimulateResults(daily, iters=101):\n",
    "    model, results = RunLinearModel(daily)\n",
    "    fake = daily.copy()\n",
    "    result_seq = []\n",
    "    for i in range(iters):\n",
    "        fake.ppg = results.fittedvalues + Resample(results.resid)\n",
    "        _, fake_results = RunLinearModel(fake)\n",
    "        result_seq.append(fake_results)\n",
    "    \n",
    "    return result_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "daily is a DataFrame containing the observed prices; iters is the number of simulations to run.\n",
    "\n",
    "SimulateResults uses RunLinearModel, to estimate the slope and intercept of the observed values.\n",
    "\n",
    "Each time through the loop, it generates a “fake” dataset by resampling the residuals and adding them to the fitted values. Then it runs a linear model on the fake data and stores the RegressionResults object.\n",
    "\n",
    "The next step is to use the simulated results to generate predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def GeneratePredictions(result_seq, years, add_resid=False):\n",
    "    n = len(years)\n",
    "    d = dict(Intercept=np.ones(n), years=years, years2=years**2)\n",
    "    predict_df = pandas.DataFrame(d)\n",
    "    predict_seq = []\n",
    "    \n",
    "    for fake_results in result_seq:\n",
    "        predict = fake_results.predict(predict_df)\n",
    "        if add_resid:\n",
    "            predict += thinkstats2.Resample(fake_results.resid, n)\n",
    "        predict_seq.append(predict)\n",
    "    return predict_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "GeneratePredictions takes the sequence of results from the previous step, as well as years, which is a sequence of floats that specifies the interval to generate predictions for, and add_resid, which indicates whether it should add resampled residuals to the straight-line prediction. GeneratePredictions iterates through the sequence of RegressionResults and generates a sequence of predictions.\n",
    "\n",
    "Finally, here’s the code that plots a 90% confidence interval for the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def PlotPredictions(daily, years, iters=101, percent=90):\n",
    "    result_seq = SimulateResults(daily, iters=iters)\n",
    "    p = (100 - percent) / 2\n",
    "    percents = p, 100-p\n",
    "    \n",
    "    predict_seq = GeneratePredictions(result_seq, years, True)\n",
    "    low, high = thinkstats2.PercentileRows(predict_seq, percents)\n",
    "    thinkplot.FillBetween(years, low, high, alpha=0.3, color='gray')\n",
    "    \n",
    "    predict_seq = GeneratePredictions(result_seq, years, False)\n",
    "    low, high = thinkstats2.PercentileRows(predict_seq, percents)\n",
    "    thinkplot.FillBetween(years, low, high, alpha=0.5, color='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "PlotPredictions calls GeneratePredictions twice: once with add_resid=True and again with add_resid=False. It uses PercentileRows to select the 5th and 95th percentiles for each year, then plots a gray region between these bounds.\n",
    "\n",
    "Use the revious code to plot the calculated predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Code it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The lighter region shows a 90% confidence interval for prediction error, which is the sum of sampling error and random variation.\n",
    "\n",
    "These regions quantify sampling error and random variation, but not modeling error. In general modeling error is hard to quantify, but in this case we can address at least one source of error, unpredictable external events.\n",
    "\n",
    "The regression model is based on the assumption that the system is **stationary**; that is, that the parameters of the model don’t change over time. Specifically, it assumes that the slope and intercept are constant, as well as\n",
    "the distribution of residuals.\n",
    "\n",
    "But looking at the moving averages in Figure 12.3, it seems like the slope changes at least once during the observed interval, and the variance of the residuals seems bigger in the first half than the second.\n",
    "\n",
    "As a result, the parameters we get depend on the interval we observe. To see how much effect this has on the predictions, we can extend SimulateResults to use intervals of observation with different start and end dates. My implementation is in timeseries.py.\n",
    "\n",
    "The model based on the entire interval has positive slope, indicating that prices were increasing. But the most recent interval shows signs of decreasing prices, so models based on the most recent data have negative slope. As a\n",
    "result, the widest predictive interval includes the possibility of decreasing prices over the next year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Time series analysis is a big topic; this chapter has only scratched the surface. An important tool for working with time series data is autoregression, which I did not cover here, mostly because it turns out not to be useful for the example data I worked with.\n",
    "\n",
    "But once you have learned the material in this chapter, you are well prepared to learn about autoregression. One resource I recommend is Philipp Janert’s xbook, Data Analysis with Open Source Tools, O’Reilly Media, 2011. His\n",
    "chapter on time series analysis picks up where this one leaves off."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
