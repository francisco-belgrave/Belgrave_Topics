{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Lesson 14 - Latent Variables and Natural Language Processing\n",
    "\n",
    "---\n",
    "\n",
    "## Guided practice and demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Config\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spacy is used for pre-processing and traditional NLP\n",
    "import spacy\n",
    "from spacy.en import English\n",
    "\n",
    "# Gensim is used for LDA and word2vec\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "df = pd.read_csv('../dataset/stumbleupon.tsv', sep='\\t')\n",
    "df['title'] = df.boilerplate.map(lambda x: json.loads(x).get('title', ''))\n",
    "df['body'] = df.boilerplate.map(lambda x: json.loads(x).get('body', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: \"LDA in gensim\"\n",
    "\n",
    "Gensim is a library of language processing tools focused on latent variable models for text. It was originally developed by grad students dissatisfied with current implementations of latent models. Documentation and tutorials are available on the [package’s website](https://radimrehurek.com/gensim/index.html).\n",
    "\n",
    "\n",
    "Let’s first translate a set of documents (articles) into a matrix representation with a row per document and a column per feature (word or n-gram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "body_text = df.body.dropna()\n",
    "vectorizer = CountVectorizer(binary=False,\n",
    "                             stop_words='english',\n",
    "                             min_df=3)\n",
    "vectorizer.fit(body_text)\n",
    "docs = vectorizer.transform(body_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cheers flagship recounting\n"
     ]
    }
   ],
   "source": [
    "# Build a mapping of numerical ID to word\n",
    "id2word = dict(enumerate(vectorizer.get_feature_names()))\n",
    "print id2word[5000], id2word[10000], id2word[20000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We want to learn which columns are correlated (i.e. likely to come from the same topic). This is the word distribution. \n",
    "- We can also determine what topics are in each document, the topic distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "\n",
    "# First we convert our word-matrix into gensim's format\n",
    "corpus = Sparse2Corpus(docs, documents_columns=False)\n",
    "\n",
    "# Then we fit an LDA model\n",
    "lda_model = LdaModel(corpus=corpus, id2word=id2word, num_topics=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model, we need to explicitly specify the number of topics we want the model to uncover. This is a critical parameter, but there isn’t much guidance on how to choose it.  Try to use domain expertise where possible.\n",
    "\n",
    "\n",
    "Now we need to assess the goodness of fit for our model. Like other unsupervised learning techniques, our validation techniques are mostly about interpretation.\n",
    "\n",
    "#### Use the following questions to guide you:\n",
    "\n",
    "- Did we learn reasonable topics?\n",
    "- Do the words that make up a topic make sense?\n",
    "- Is this topic helpful towards our goal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can evaluate fit by viewing the top words in each topic.\n",
    "\n",
    "- Gensim has a `show_topics()` function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      "(3, u'0.007*\"10\" + 0.006*\"said\" + 0.006*\"just\" + 0.005*\"2009\" + 0.005*\"like\" + 0.005*\"2010\" + 0.004*\"12\" + 0.004*\"11\" + 0.004*\"game\" + 0.004*\"pm\"')\n",
      "\n",
      "Topic: 1\n",
      "(1, u'0.006*\"news\" + 0.006*\"workout\" + 0.005*\"exercises\" + 0.005*\"muscle\" + 0.004*\"like\" + 0.004*\"said\" + 0.004*\"just\" + 0.004*\"leg\" + 0.004*\"body\" + 0.004*\"make\"')\n",
      "\n",
      "Topic: 2\n",
      "(10, u'0.017*\"cake\" + 0.011*\"dress\" + 0.010*\"sleep\" + 0.008*\"chocolate\" + 0.007*\"00\" + 0.004*\"time\" + 0.004*\"make\" + 0.003*\"wedding\" + 0.003*\"carnival\" + 0.003*\"und\"')\n",
      "\n",
      "Topic: 3\n",
      "(8, u'0.007*\"health\" + 0.005*\"people\" + 0.005*\"body\" + 0.005*\"like\" + 0.004*\"food\" + 0.004*\"cancer\" + 0.004*\"day\" + 0.004*\"time\" + 0.004*\"help\" + 0.003*\"water\"')\n",
      "\n",
      "Topic: 4\n",
      "(13, u'0.013*\"com\" + 0.009*\"http\" + 0.008*\"www\" + 0.006*\"new\" + 0.005*\"href\" + 0.005*\"best\" + 0.005*\"content\" + 0.004*\"left\" + 0.004*\"cbssports\" + 0.004*\"like\"')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ti, topic in enumerate(lda_model.show_topics(num_topics=5, num_words=10)):\n",
    "    print \"Topic: {}\".format(ti)\n",
    "    print topic\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's now use our fitted model to predict topics for some new data\n",
    "\n",
    "(examples taken from http://www.buzzfeed.com/babymantis/25-stupid-newspaper-headlines-1opu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8461, 1), (9477, 1), (10556, 1), (11436, 1), (13429, 1), (21460, 1)]\n",
      "[(u'ears', 1), (u'eyes', 1), (u'frog', 1), (u'grow', 1), (u'japanese', 1), (u'scientists', 1)]\n"
     ]
    }
   ],
   "source": [
    "new_text = [\n",
    "    \"Japanese scientists grow frog eyes and ears\",\n",
    "    \"Statistics show that teen pregnancy drops of significantly after age 25\",\n",
    "    \"Bugs flying around with wings are flying bugs\",\n",
    "    \"Federal agents raid gun shop, find weapons\",\n",
    "    \"Marijuana issue sent to a joint committee\"\n",
    "]\n",
    "\n",
    "# Transform the text into the bag-of-words (bow) space using our vectorizer\n",
    "new_bow = vectorizer.transform(new_text)\n",
    "\n",
    "# Transform into format expected by gensim\n",
    "new_corpus = Sparse2Corpus(new_bow, documents_columns=False)\n",
    "\n",
    "# Print out first entry + matching words\n",
    "print list(new_corpus)[0]\n",
    "print [(id2word[id], count) for id, count in list(new_corpus)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform into LDA space by applying fitted LDA model to the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_vector = lda_model[new_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each entry we can extract a tuple indicating how much it makes part of each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(8, 0.63181306448865238), (10, 0.24437718882330839)],\n",
       " [(5, 0.3336190844178899), (8, 0.55804729894211291)],\n",
       " [(0, 0.011111132890035902),\n",
       "  (1, 0.011111138793326703),\n",
       "  (2, 0.011111128325934191),\n",
       "  (3, 0.011111116928335189),\n",
       "  (4, 0.011111116289867936),\n",
       "  (5, 0.22317720606119099),\n",
       "  (6, 0.011111132516016461),\n",
       "  (7, 0.011111116850011184),\n",
       "  (8, 0.011111114308550711),\n",
       "  (9, 0.011111119703747542),\n",
       "  (10, 0.011111128722263669),\n",
       "  (11, 0.011111121504129736),\n",
       "  (12, 0.6323781684114721),\n",
       "  (13, 0.011111128392975156),\n",
       "  (14, 0.011111130302142516)],\n",
       " [(12, 0.86666618543879392)],\n",
       " [(0, 0.011111122439168298),\n",
       "  (1, 0.011111133366836626),\n",
       "  (2, 0.011111183859447316),\n",
       "  (3, 0.011111136356965843),\n",
       "  (4, 0.01111113363404132),\n",
       "  (5, 0.011111128024102992),\n",
       "  (6, 0.011111125249237261),\n",
       "  (7, 0.84444412400051205),\n",
       "  (8, 0.011111136845392298),\n",
       "  (9, 0.011111116052744388),\n",
       "  (10, 0.011111129296949211),\n",
       "  (11, 0.011111127448963865),\n",
       "  (12, 0.011111137238477207),\n",
       "  (13, 0.01111112862796741),\n",
       "  (14, 0.011111137559193796)]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[list(lda_vec) for lda_vec in lda_vector]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract most prominent LDA topics for each entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 0.63178064645276466),\n",
       " (8, 0.5567098314508202),\n",
       " (12, 0.63233605463090126),\n",
       " (12, 0.86666626907355926),\n",
       " (7, 0.84444391572062794)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_topics = [max(x, key=lambda item: item[1]) for x in list(lda_vector)]\n",
    "top_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print out text + topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japanese scientists grow frog eyes and ears\n",
      "63.2% as topic #8:\n",
      "0.007*\"health\" + 0.005*\"people\" + 0.005*\"body\" + 0.005*\"like\" + 0.004*\"food\" + 0.004*\"cancer\" + 0.004*\"day\" + 0.004*\"time\" + 0.004*\"help\" + 0.003*\"water\" \n",
      "\n",
      "Statistics show that teen pregnancy drops of significantly after age 25\n",
      "55.7% as topic #8:\n",
      "0.007*\"health\" + 0.005*\"people\" + 0.005*\"body\" + 0.005*\"like\" + 0.004*\"food\" + 0.004*\"cancer\" + 0.004*\"day\" + 0.004*\"time\" + 0.004*\"help\" + 0.003*\"water\" \n",
      "\n",
      "Bugs flying around with wings are flying bugs\n",
      "63.2% as topic #12:\n",
      "0.005*\"world\" + 0.005*\"new\" + 0.004*\"000\" + 0.003*\"year\" + 0.003*\"just\" + 0.003*\"like\" + 0.003*\"said\" + 0.003*\"time\" + 0.003*\"years\" + 0.002*\"says\" \n",
      "\n",
      "Federal agents raid gun shop, find weapons\n",
      "86.7% as topic #12:\n",
      "0.005*\"world\" + 0.005*\"new\" + 0.004*\"000\" + 0.003*\"year\" + 0.003*\"just\" + 0.003*\"like\" + 0.003*\"said\" + 0.003*\"time\" + 0.003*\"years\" + 0.002*\"says\" \n",
      "\n",
      "Marijuana issue sent to a joint committee\n",
      "84.4% as topic #7:\n",
      "0.008*\"2010\" + 0.007*\"2008\" + 0.007*\"2007\" + 0.006*\"2009\" + 0.006*\"2006\" + 0.005*\"2011\" + 0.004*\"new\" + 0.004*\"october\" + 0.004*\"health\" + 0.004*\"2005\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, topic_tuple in enumerate(top_topics):\n",
    "    print new_text[i]\n",
    "    print \"{0:.1f}% as topic #{1}:\".format(100 * topic_tuple[1], topic_tuple[0])\n",
    "    print lda_model.print_topic(topic_tuple[0],topn=10), \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more examples on using LDA with gensim, see: http://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Word2Vec in gensim\n",
    "\n",
    "We will build a Word2Vec model using the text body of the articles available in the StumbleUpon dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Word2Vec class has many arguments:\n",
    "\n",
    "- size represents how many concepts or topics we should use\n",
    "- window represents how many words surrounding a sentence we should use as our original feature\n",
    "- min_count is the number of times that context or word must appear\n",
    "- workers is the number of CPU cores to use to speed up model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Setup the text body\n",
    "text = df.body.dropna().map(lambda x: x.split())\n",
    "model = Word2Vec(text,\n",
    "                 size=100,      # how many concepts or topics should we use?\n",
    "                 window=5,      # how many words surrounding a sentence we should use as our original feature?\n",
    "                 min_count=5,   # number of times that context or word must appear\n",
    "                 workers=4)     # number of CPU cores to use (can speed up model training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has a `most_similar function` that helps finding the words most similar to the one you queried.\n",
    "This will return words that are most often used in the same context.\n",
    "It can easily identify words related to those from this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'cupcake', 0.9082363843917847),\n",
       " (u'pie', 0.8527567982673645),\n",
       " (u'candy', 0.8522288799285889),\n",
       " (u'crust', 0.8520104885101318),\n",
       " (u'tart', 0.828007698059082),\n",
       " (u'cheesecake', 0.8274497389793396),\n",
       " (u'cake', 0.8255833387374878),\n",
       " (u'icing', 0.8201949596405029),\n",
       " (u'mini', 0.8160595297813416),\n",
       " (u'buttercream', 0.8136184811592102)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['cookie', 'brownie'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word vector maths: \n",
    "\n",
    "- \"man - boy $\\approx$ person\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'sportsmanship', 0.5067369937896729),\n",
       " (u'person', 0.45713984966278076),\n",
       " (u'people', 0.3930201530456543),\n",
       " (u'example', 0.38871055841445923),\n",
       " (u'difference', 0.38625895977020264),\n",
       " (u'Americans', 0.3604770302772522),\n",
       " (u'those', 0.3589774966239929),\n",
       " (u'solution', 0.34824103116989136),\n",
       " (u'consumers', 0.3280426263809204),\n",
       " (u'anyone', 0.32233816385269165)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['man'], negative=['boy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read this as \"man is to woman as boy is to...girl\"\n",
    " \n",
    "- \"man + boy - woman = girl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'daughter', 0.768936038017273),\n",
       " (u'girl', 0.7630077004432678),\n",
       " (u'brother', 0.7546021938323975),\n",
       " (u'wife', 0.7309377193450928),\n",
       " (u'girlfriend', 0.7301152944564819),\n",
       " (u'father', 0.7230810523033142),\n",
       " (u'son', 0.7200206518173218),\n",
       " (u'guy', 0.6960198879241943),\n",
       " (u'shot', 0.6947152018547058),\n",
       " (u'caught', 0.6867551207542419)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['man', 'boy'], negative=['woman'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"cheesecake + cake - frosting = pie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'pie', 0.7631522417068481),\n",
       " (u'crust', 0.738057553768158),\n",
       " (u'tart', 0.7293705940246582),\n",
       " (u'pizza', 0.7125093340873718),\n",
       " (u'brownie', 0.7053690552711487),\n",
       " (u'dessert', 0.6859625577926636),\n",
       " (u'cookie', 0.6765167117118835),\n",
       " (u'cupcake', 0.6735799312591553),\n",
       " (u'brownies', 0.6691722273826599),\n",
       " (u'recipe', 0.6645998954772949)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['cheesecake', 'cake'], negative=['frosting'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data + science - statistics = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'device', 0.725189745426178),\n",
       " (u'technology', 0.7006625533103943),\n",
       " (u'product', 0.681377649307251),\n",
       " (u'company', 0.6799347996711731),\n",
       " (u'industry', 0.6791709661483765),\n",
       " (u'material', 0.6722677946090698),\n",
       " (u'development', 0.6717053651809692),\n",
       " (u'human', 0.6707926392555237),\n",
       " (u'research', 0.6674562692642212),\n",
       " (u'virus', 0.6576155424118042)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['data', 'science'], negative=['statistics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### technology + entrepreneur - hipster = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'design', 0.8124440312385559),\n",
       " (u'electronics', 0.807840883731842),\n",
       " (u'innovative', 0.7850945591926575),\n",
       " (u'concept', 0.7809066772460938),\n",
       " (u'manufacturing', 0.7697283029556274),\n",
       " (u'technologies', 0.7624959945678711),\n",
       " (u'phones', 0.7618154287338257),\n",
       " (u'solar', 0.7546247839927673),\n",
       " (u'military', 0.7517671585083008),\n",
       " (u'international', 0.750795841217041)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['technology', 'entrepreneur'], negative=['hipster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which one of these doesn't fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cereal\n",
      "myspace\n"
     ]
    }
   ],
   "source": [
    "print model.doesnt_match(\"breakfast cereal lunch dinner\".split())\n",
    "print model.doesnt_match(\"facebook twitter tumblr myspace\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity between two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.903090449011\n",
      "0.226632053199\n",
      "0.620058241646\n",
      "-0.0905850081333\n"
     ]
    }
   ],
   "source": [
    "print model.similarity('man', 'woman')\n",
    "print model.similarity('man', 'monkey')\n",
    "print model.similarity('apple', 'pear')\n",
    "print model.similarity('man', 'apple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect a single vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.48491478,  0.82271338, -0.66203606,  1.47417819, -0.83262259,\n",
       "       -0.58856583,  0.70644802,  0.6183418 ,  0.83599722, -0.32860628,\n",
       "        1.01383781,  0.27052182, -0.91036075,  0.86578614, -1.10889387,\n",
       "        0.32364827, -0.07446388, -0.83384198, -0.85951984, -0.73046398,\n",
       "       -0.06517454, -1.00799024,  1.32200027,  1.04098773, -0.21395648,\n",
       "        0.21387762,  1.23757994, -1.03043449, -0.27736598,  1.13346863,\n",
       "        0.2824986 ,  0.82115883,  0.99513644, -0.20105761, -0.07797236,\n",
       "       -0.56093013,  0.61872196, -0.6510182 , -0.85838211,  0.15905283,\n",
       "       -0.955392  , -0.63124895, -0.2876226 ,  0.84796757,  0.40757838,\n",
       "       -1.34304857,  0.37937742,  0.83164185, -1.11950517,  0.01291989,\n",
       "        0.78509122, -0.43319365,  0.93026209, -0.17269517, -0.33601609,\n",
       "       -0.23730457,  1.97847748, -0.54382747, -0.35669383, -0.86995107,\n",
       "       -0.15285942, -0.13247661, -0.91331434, -1.90528333,  0.37306562,\n",
       "       -0.33279184,  0.66351765, -1.06753397,  1.18373585,  0.60997862,\n",
       "        0.13878177,  0.12156512, -0.17558073,  1.22159481,  0.5217185 ,\n",
       "       -0.6171968 , -0.22267435,  0.44891614, -0.84637272, -0.2363719 ,\n",
       "       -0.59885728,  2.31772089,  0.831792  ,  0.46590021, -0.35120356,\n",
       "        0.53920954, -0.7383393 , -0.32599434,  0.87675017,  1.83234751,\n",
       "       -0.74155492, -0.49280098,  1.99077296,  1.99848652, -0.5029406 ,\n",
       "        0.11972249,  0.83286047, -0.32789886,  0.59089351, -1.13832867], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['man']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
